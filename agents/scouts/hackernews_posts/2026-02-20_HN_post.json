{
  "Show HN: Global Issue Memory MCP – Stack Overflow for Your Coding Assistant": {
    "date": "2026-02-20T07:44:57Z",
    "url": "https://news.ycombinator.com/item?id=47084943",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "Hey HN, I built Global Issue Memory (GIM), an open-source MCP server that lets AI coding assistants look up and share solutions to errors they encounter.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;timho102003&#x2F;global-issue-memory\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;timho102003&#x2F;global-issue-memory</a>\nTry it: <a href=\"https:&#x2F;&#x2F;www.usegim.com&#x2F;docs&#x2F;getting-started\" rel=\"nofollow\">https:&#x2F;&#x2F;www.usegim.com&#x2F;docs&#x2F;getting-started</a> (free, no signup required to use the MCP tools)<p>How I got here:<p>I do a lot of vibe coding where I prompt Claude Code and let it run autonomously. I kept noticing the same pattern: the AI would hit a common error, spend 30K+ tokens doing web searches and trying failed fixes, and by the time it found the answer the context window was so bloated that output quality had degraded. I started calling this &quot;context rot.&quot;<p>My first attempt at fixing this was having the AI store solved issues in local markdown files and check them before debugging. It worked for my own projects but obviously didn&#x27;t scale.<p>Then I realized this is basically the same problem Stack Overflow solved for humans. The difference is AI assistants can&#x27;t efficiently parse discussion threads. They need structured, machine-readable fixes.<p>What it does:<p>GIM is an MCP server with five tools. When an AI hits an error, it calls gim_search_issues and gets back a verified fix in ~500 tokens instead of burning 30K on web searches. When the AI solves something new, it can submit the solution back via gim_submit_issue. Solutions are deduplicated and matched using semantic search.<p>Knowledge comes from two paths. First, a GitHub issue monitoring pipeline that crawls 60+ popular repos (LangChain, FastAPI, Next.js, etc.) and tracks closed issues with merged PRs linked. Those get automatically extracted, sanitized, and stored. Second, user contributions through the MCP tools, where an AI session solves something with a workaround and contributes it back.<p>Both paths feed into the same knowledge base, and here&#x27;s where it gets interesting. Say a user hits a bug and their workaround gets logged. Later, when the crawler picks up the maintainer&#x27;s official fix, it automatically identifies the overlap, merges, and deduplicates so the knowledge base stays clean and up to date. You get immediate workarounds to unblock you now, and the official fix once it lands.<p>Technical details:<p>Built with FastAPI, Qdrant for vector search, and Supabase. The MCP integration follows the Model Context Protocol spec so it works with any MCP-compatible client.<p>Privacy:<p>This was non-negotiable for me. If people are going to contribute solutions from their codebases, they need to trust that nothing sensitive leaks. Every submission passes through two layers of sanitization. First, regex pattern matching catches 20+ known secret types (API keys, tokens, connection strings) and scrubs PII. Second, an LLM-powered pass reviews the content in context to catch things that don&#x27;t match any regex pattern, like hardcoded credentials in unusual formats. And because no sanitization system can claim to catch everything, the entire codebase is open source so anyone can audit it and open a PR if they spot a gap.<p>Why open source and community matter here:<p>The value of GIM scales directly with contributions. One AI solves a bug, every AI learns the fix. That only works if people trust it enough to participate, which is why open source isn&#x27;t just a nice-to-have, it&#x27;s the foundation. I bootstrapped from 60+ repos to make it useful on day one, but the real potential is the community filling in the long tail of issues that no single team could ever catalog.<p>On the roadmap, I&#x27;m planning to add MCP tools that let Claude directly raise issues in upstream repos when it detects something that needs maintainer attention, and also let users add their own repos to be tracked by the crawler. So the coverage keeps growing with the community.<p>The whole thing is open source: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;timho102003&#x2F;global-issue-memory\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;timho102003&#x2F;global-issue-memory</a><p>Happy to answer any questions about the architecture, the sanitization approach, or anything else.",
    "comments": []
  },
  "Show HN: Mcpsec-A multi-agent SEC gate for MCP toolchains (scan →harden →rescan)": {
    "date": "2026-02-19T21:41:57Z",
    "url": "https://news.ycombinator.com/item?id=47079889",
    "matched_keywords": [
      "Model Context Protocol"
    ],
    "post": "Hi HN,<p>I built MCPSEC, a security gatekeeper for MCP (Model Context Protocol) toolchains.<p>It scans MCP configs, correlates vulnerability intel (OSV &#x2F; GHSA &#x2F; NVD), simulates tool abuse with an LLM-based probe agent, generates a policy + patch plan, applies hardening, then re-scans and gates CI on the final risk score.<p>The design is intentionally agentic:\n- Inventory agent: parses MCP configs\n- Intel agent: pulls vuln data (OSV &#x2F; GHSA &#x2F; NVD)\n- Probe agent (LLM, optional): generates adversarial tool abuse prompts\n- Policy agent (LLM, optional): turns findings into concrete config changes\n- Orchestrator: merges results, scores risk, writes reports, applies patches<p>You can run it locally as a CLI or drop it into CI as a GitHub Action:\n- It produces before&#x2F;apply&#x2F;after reports as artifacts\n- It can fail PRs if the final risk score stays above a threshold\n- Without an LLM token it works as a deterministic scanner; with one it becomes a true “security copilot”<p>Repo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;yuvrajgitwork&#x2F;MCP-toolchain-security-GK\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;yuvrajgitwork&#x2F;MCP-toolchain-security-GK</a>  \nDemo workflow: scan → apply → rescan → lower score<p>I built this because MCP toolchains are becoming powerful and over-privileged very quickly, and there’s basically no security gate for them yet.<p>Would love feedback from folks working in AI infra &#x2F; security.",
    "comments": []
  },
  "Show HN: Messaging without accounts – continuity-authenticated instead": {
    "date": "2026-02-19T20:09:22Z",
    "url": "https://news.ycombinator.com/item?id=47078523",
    "matched_keywords": [
      "Model Context Protocol"
    ],
    "post": "Most messaging systems are account systems first and cryptosystems second.<p>I&#x27;ve been working on a prototype: https:&#x2F;&#x2F;aq-message.web.app<p>AQ is an experiment in removing accounts entirely.<p>There are no usernames, phone numbers, or platform identity objects. Instead, each relationship maintains its own evolving continuity chain. A message is accepted only if it cryptographically advances that chain according to deterministic rules.<p>The relay is untrusted by design. It stores ciphertext under per-relationship mailboxes. It can observe timing and volume, but it cannot decrypt content or produce forged messages that would be admitted by recipients.<p>Notable properties:\n• No global identity namespace\n• No server-issued credentials\n• Pairwise continuity instead of platform authentication\n• Deterministic replay and fork handling\n• Explicit recovery triggers on continuity gaps\n• No claim of metadata anonymity<p>This is a prototype, not a polished product.<p>I’d appreciate scrutiny on the protocol mechanics and threat model.<p>Technical overview:\nhttps:&#x2F;&#x2F;aq-message.web.app&#x2F;technical-overview",
    "comments": []
  },
  "Show HN: Claude Code for Mobile GUI Automation": {
    "date": "2026-02-19T12:44:18Z",
    "url": "https://news.ycombinator.com/item?id=47073173",
    "matched_keywords": [
      "Model Context Protocol"
    ],
    "post": "Phone GUI agents (e.g., AutoGLM-Phone, GELab) can already do NL-driven taps&#x2F;navigation&#x2F;form filling.\n  My observation: smaller GUI models (often 4B&#x2F;9B class) work well for single interactions, but become brittle on long workflows with branching and recovery.<p><pre><code>  I built a Skill layer that separates planning from execution:\n\n  - Planner: Claude Code &#x2F; Codex (task decomposition, decision-making, replanning)\n  - Orchestrator: Skill layer (state machine, retries&#x2F;rollback, tool protocol)\n  - Executor: phone GUI model (screen parsing + UI actions + cross-app execution)\n\n  Execution loop:\n\n  1. Goal in NL&#x2F;template\n  2. Planner emits step plan + conditions + fallback strategy\n  3. Skill compiles into atomic actions (tap&#x2F;type&#x2F;swipe&#x2F;wait&#x2F;verify)\n  4. GUI executor runs on real&#x2F;cloud phone, returns screenshots&#x2F;state&#x2F;structured output\n  5. Planner&#x2F;orchestrator decide next step until success&#x2F;fallback\n Potential use cases:\n\n  - recruiting outreach automation\n  - multi-platform content distribution\n  - social outreach workflows\n  - lead extraction\n  - competitor monitoring</code></pre>",
    "comments": []
  },
  "Show HN: Open-source security scanner for MCP (Model Context Protocol) servers": {
    "date": "2026-02-19T12:32:17Z",
    "url": "https://news.ycombinator.com/item?id=47073091",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "MCP servers let AI assistants (Claude, Copilot, Cursor) interact with databases, APIs, and filesystems. I&#x27;ve been reviewing a lot of these — both open-source and internal — and keep finding the same issues: hardcoded API keys, eval() on user input, SQL injection via string concatenation, wildcard permissions, disabled TLS.<p>So I built a static analysis scanner specifically for MCP servers. It runs 7 analyzers (secrets, static code, prompt injection, SQL&#x2F;command injection, permissions, network, dependencies) and takes ~45ms on a typical server.<p>Usage:<p><pre><code>  npx mcp-security-auditor scan .&#x2F;my-mcp-server\n</code></pre>\nNo account, runs locally. Outputs text, JSON, SARIF (for GitHub Security tab), HTML, or Markdown. Has a CI mode that exits non-zero above a severity threshold.<p>Available on both npm and PyPI. MIT licensed.<p>npm: <a href=\"https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;mcp-security-auditor\" rel=\"nofollow\">https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;mcp-security-auditor</a>\nPyPI: <a href=\"https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;mcp-security-auditor&#x2F;\" rel=\"nofollow\">https:&#x2F;&#x2F;pypi.org&#x2F;project&#x2F;mcp-security-auditor&#x2F;</a>\nDev.to writeup with examples: <a href=\"https:&#x2F;&#x2F;dev.to&#x2F;prabhu_raja_fe2261464cb8e&#x2F;how-to-scan-your-mcp-servers-for-security-vulnerabilities-in-10-seconds-4m59\" rel=\"nofollow\">https:&#x2F;&#x2F;dev.to&#x2F;prabhu_raja_fe2261464cb8e&#x2F;how-to-scan-your-mc...</a><p>Would love feedback on detection patterns — there are definitely gaps I haven&#x27;t covered yet.",
    "comments": []
  },
  "Show HN: One async PHP process serving web, REST API, and MCP for AI agents": {
    "date": "2026-02-20T10:25:23Z",
    "url": "https://news.ycombinator.com/item?id=47086117",
    "matched_keywords": [
      "MCP server"
    ],
    "post": "",
    "comments": []
  },
  "Show HN: 17MB pronunciation scorer beats human experts at phoneme level": {
    "date": "2026-02-20T09:57:37Z",
    "url": "https://news.ycombinator.com/item?id=47085899",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "I built an English pronunciation assessment engine that fits in 17MB and runs in under 300ms on CPU.<p>Architecture: CTC forced alignment + GOP scoring + ensemble heads (MLP + XGBoost). No wav2vec2 or large self-supervised models — the entire pipeline uses a quantized NeMo Citrinet-256 as the acoustic backbone.<p>Benchmarked on speechocean762 (standard academic benchmark, 2500 utterances):\n- Phone accuracy (PCC): 0.580 — exceeds human inter-annotator agreement (0.555)\n- Sentence accuracy: 0.710 — exceeds human agreement (0.675)\n- Model is 70x smaller than wav2vec2-based SOTA<p>Trade-off: we&#x27;re ~10-15% below SOTA on raw accuracy. But for real-time feedback in language learning apps, the latency&#x2F;size trade-off is worth it.<p>Available as REST API, MCP server (for AI agents), and on Azure Marketplace.<p>Demo: <a href=\"https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;fabiosuizu&#x2F;pronunciation-assessment\" rel=\"nofollow\">https:&#x2F;&#x2F;huggingface.co&#x2F;spaces&#x2F;fabiosuizu&#x2F;pronunciation-asses...</a><p>Interested in feedback on the scoring approach and use cases people would find valuable.",
    "comments": []
  },
  "Show HN: Inkwell-MCP – An MCP server for newsletter creators (open source)": {
    "date": "2026-02-20T08:19:40Z",
    "url": "https://news.ycombinator.com/item?id=47085202",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "",
    "comments": [
      {
        "author": "lbostral",
        "text": "I&#x27;ve been writing a weekly streaming industry newsletter for 68 editions. Most of my time wasn&#x27;t writing — it was information logistics: tracking sources, searching my own archive, remembering which expert I&#x27;d cited and when.\nBookmarking tools keep dying (RIP Pocket). If you write a newsletter, your sources are your most valuable asset and they live in tools you don&#x27;t control. I built an MCP server to own that layer — sources, notes, briefs, article tracking. SQLite-backed, your data stays yours.<p>The design principle: the MCP handles everything around the writing (collecting, organizing, retrieving). It never generates content.<p>Caveat: I daily-drive a more advanced version (Supabase, custom schema, 50 tools) to build two paid industry reports (20K+ datapoints, 172 companies, 34 countries). inkwell-mcp is the extracted portable core. It type-checks, but I haven&#x27;t battle-tested it with anyone else&#x27;s workflow yet. Issues welcome.\n25 years in streaming, built M6 Replay (50M+ users), several CTO roles.\nFull write-up: Full write-up: <a href=\"https:&#x2F;&#x2F;streamingradar.substack.com&#x2F;p&#x2F;my-newsletter-has-50-ai-tools-the\" rel=\"nofollow\">https:&#x2F;&#x2F;streamingradar.substack.com&#x2F;p&#x2F;my-newsletter-has-50-a...</a>\nThe newsletter: <a href=\"https:&#x2F;&#x2F;www.streaming-radar.com\" rel=\"nofollow\">https:&#x2F;&#x2F;www.streaming-radar.com</a>\nThe full MCP with 20K+ datapoints (bidirectional, 4 slots): <a href=\"https:&#x2F;&#x2F;lens.streaming-radar.com&#x2F;mcp\" rel=\"nofollow\">https:&#x2F;&#x2F;lens.streaming-radar.com&#x2F;mcp</a>\nHappy to answer questions about the architecture or MCP design decisions.",
        "created_at": "2026-02-20T08:30:06.000Z"
      }
    ]
  },
  "Ask HN: What Is the Point of WebMCP?": {
    "date": "2026-02-20T08:00:29Z",
    "url": "https://news.ycombinator.com/item?id=47085076",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "So I have a question that should be simple, but apparently is not. I have Chrome Canary with WebMCP enabled. I have the Model Context Tool extension installed. I have a WebMCP-enabled test app running in it.<p>The only thing the extension offers is connecting to the Gemini API via an API token and using that as the driver for the WebMCP app. I don&#x27;t understand this use case. If I wanted my app to consume API tokens I would have directly coded interoperation with the major providers with one of the thousands libraries that di exactly that.<p>What I do have are subscriptions to OpenAI, Gemini, Claude, and GitHub Copilot. All the clients I use can connect to MCP servers. I thought: I have WebMCP enabled in Canary, so my browser will actually be an MCP server that gives access to the underlying WebMCP apps it is running. It turns out that no, of course that would be logical, but you can&#x27;t do that. Chrome exposes an MCP server, but it is only for controlling Chrome itself, like Playwright. It does not provide any bridging to WebMCP.<p>I then thought there would be an extension that does this, but the Chrome extension ecosystem is full of bullshit extensions from fraudsters who vibe-coded their way into trying to capitalize on the AI craze. There are no extensions for that. All of them want you to subscribe, or give them API tokens. Many of them have non-working websites. One option I see mentioned online is running Chrome in debug mode and exposing a remote debugging port. This is exactly what WebMCP was supposedly invented to avoid, so running your browser in a special way to expose it to complete control cannot be the right way to do this. I&#x27;m perfectly able to already do that without WebMCP.<p>Am I understanding it wrong? I understand the proposal is in its infancy, but I expect the <i>goal</i> of it to be at least clear. I don&#x27;t understand where it helps developers or users in any single way that wasn&#x27;t already available before.",
    "comments": [
      {
        "author": "BlueHotDog2",
        "text": "WebMCP is a protocol for exposing tools the AI can call from your running web app.\nthe point isn&#x27;t &quot;consume API tokens&quot;, it&#x27;s &quot;let the AI do stuff in your app&quot; (click buttons, fill forms, read DOM state). The Gemini integration is just the orchestrator for the example implementation. not the protocol",
        "created_at": "2026-02-20T08:58:01.000Z"
      },
      {
        "author": "curtisblaine",
        "text": "Yes, but in practice, how do you connect AI to WebMCP? What is the point of a protocol that speaks MCP inside the browser if it&#x27;s not reachable outside the browser?",
        "created_at": "2026-02-20T09:21:45.000Z"
      },
      {
        "author": "ai_tools_daily",
        "text": "The value of WebMCP becomes clearer when you think about non-developers trying to connect AI to their existing tools. Right now if a small business owner wants an AI agent to interact with their CRM or booking system, they need a developer to build custom integrations. A standardized protocol for web-based tool access could make that accessible to people without engineering teams. Whether WebMCP specifically is the right approach is debatable, but the problem it&#x27;s trying to solve is real.",
        "created_at": "2026-02-20T10:53:58.000Z"
      }
    ]
  },
  "Show HN: Free web search for AI agents via MCP (You.com)": {
    "date": "2026-02-20T01:41:56Z",
    "url": "https://news.ycombinator.com/item?id=47082612",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "Hey HN — I’m a PM at You.com.<p>We just made our web search API free to use via MCP, so AI agents can call real-time web search without managing API keys or billing.<p>If you’re building with Cursor, Claude Desktop, OpenAI, or your own MCP-compatible agent, you can plug in our MCP server and get:\n • Real-time web results\n • Structured answers (news, web, RAG-friendly output)\n • No cost for search via MCP<p>Docs are here:\n<a href=\"https:&#x2F;&#x2F;docs.you.com&#x2F;developer-resources&#x2F;mcp-server\" rel=\"nofollow\">https:&#x2F;&#x2F;docs.you.com&#x2F;developer-resources&#x2F;mcp-server</a><p>Would love feedback from folks building agentic workflows:\n • What’s missing?\n • What would make this actually useful in production?\n • Where does search break in your current stack?",
    "comments": []
  },
  "AI Skills Platform (Stealth) – Technical Co-Founder – Remote (US) – Equity": {
    "date": "2026-02-20T00:12:07Z",
    "url": "https://news.ycombinator.com/item?id=47081777",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "Building the &quot;Shopify for AI expertise&quot;. This is a platform where domain experts create AI skills once and deploy everywhere: web apps, Claude&#x2F;ChatGPT via MCP, APIs.\nWorking prototype with 40++ live skills, MCP server integration, streaming execution.<p>Looking for a technical co-founder to own the architecture and engineering. This is a ground-floor equity role, not a salaried position.<p>You: 4-8 years experience. Full-stack (Next.js&#x2F;React&#x2F;TypeScript preferred). Hands-on with LLM APIs (Anthropic, OpenAI). You&#x27;ve shipped AI-native products to real users. You care about system design — multi-tenant SaaS, token-metered billing, marketplace dynamics. You want to build a company, not just write code.<p>Us: Serial founder (4 companies, 3 acquired). Invented WinSock. Founding member of Intel&#x27;s AI Group ($1B+ revenue). Business co-founder with enterprise GTM and $1.2B revenue program experience from Intel.<p>You&#x27;d be building at the intersection of every major AI platform — Anthropic, OpenAI, Gemini — not locked into one ecosystem.<p>martin.hall.kp@gmail.com\nhttps:&#x2F;&#x2F;calendly.com&#x2F;martin-hall-kp&#x2F;co-founder-chat",
    "comments": []
  },
  "Show HN:PolyMCP Python Tools with Autonomous Agents": {
    "date": "2026-02-19T22:07:49Z",
    "url": "https://news.ycombinator.com/item?id=47080258",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "PolyMCP is a framework that exposes functions (in Python or TypeScript) as MCP tools, serves them through standardized MCP servers, and orchestrates them with autonomous agents capable of planning and executing complex workflows.<p>With PolyMCP, you can:\n 1. Expose existing functions (in Python or TypeScript) as MCP tools without rewriting the code.\n 2. Use the PolyClaw agent to dynamically orchestrate tools, break down complex tasks, and adapt to failures, all running in isolated environments via Docker.<p>PolyMCP is ideal for enterprise automation, DevOps, data pipelines, and internal tool orchestration.<p>GitHub Repo: https:&#x2F;&#x2F;github.com&#x2F;poly-mcp&#x2F;PolyMCP",
    "comments": []
  },
  "Show HN: Ghost OS – Let AI agents use your Mac, not just the terminal": {
    "date": "2026-02-19T19:51:38Z",
    "url": "https://news.ycombinator.com/item?id=47078272",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "I use Claude Code every day. It can write code, run tests, search files, but it can&#x27;t click a button, read what&#x27;s on screen, or send an email. It&#x27;s stuck in the terminal.<p>Ghost OS fixes this. It&#x27;s an MCP server that gives any AI agent access to every app on your Mac, not just browsers. Native apps, Electron apps, menu bars, Finder, everything.<p>Most computer-use tools rely on screenshots and pixel guessing, or require Docker containers and sandboxed browsers. Ghost OS takes a different approach: it reads the macOS accessibility tree, which the OS provides for every app. Structured data about every button, text field, and link. Faster, lighter, and works with apps that never touch a browser.<p>When the AI figures out a workflow (send a Gmail, post on Slack, download a paper), it saves it as a JSON recipe. Plain text, auditable, version controllable. A frontier model learns it once, a small model replays it forever.<p><pre><code>  brew install ghostwright&#x2F;ghost-os&#x2F;ghost-os &amp;&amp; ghost setup\n</code></pre>\nTwo commands. No Docker, no sandboxes, no container overhead. Works with Claude Code, but the MCP protocol means any agent can plug in. MIT licensed, written in Swift. Built on steipete&#x27;s AXorcist for the accessibility layer.<p>Happy to answer questions.",
    "comments": []
  },
  "Show HN: Giving Claude Code persistent memory with a self-hosted MCP server": {
    "date": "2026-02-19T19:33:09Z",
    "url": "https://news.ycombinator.com/item?id=47078061",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "",
    "comments": [
      {
        "author": "elvismdev",
        "text": "Author here. I built this because Claude Code&#x27;s built-in memory (CLAUDE.md, Auto Memory, Session Memory) works well for static rules and compressed summaries, but loses the detailed reasoning behind decisions between sessions.<p>The server uses mem0ai as a library, Qdrant for vector storage, and Ollama for local embeddings (bge-m3, 1024 dims). Optional Neo4j adds a knowledge graph. Everything including the LLM fact extraction step can run locally.<p>A few engineering decisions worth mentioning:<p>- Zero-config auth: reads your existing OAT token from ~&#x2F;.claude&#x2F;.credentials.json instead of requiring a separate API key. 3-tier fallback chain (env var → credentials file → API key).<p>- If you enable the graph layer, each add_memory triggers 3 extra LLM calls. To avoid burning your Claude subscription quota on entity extraction, you can route those to a local Ollama model (Qwen3:14b at Q4_K_M gets 0.971 tool-calling F1).<p>I patched mem0 upstream to support OAT token reuse (PR #4035). Their official MCP server is cloud-only, which is why I built this local version.<p>Happy to discuss the architecture or tradeoffs.<p>Full writeup with setup guide: <a href=\"https:&#x2F;&#x2F;dev.to&#x2F;n3rdh4ck3r&#x2F;how-to-give-claude-code-persistent-memory-with-a-self-hosted-mem0-mcp-server-h68\" rel=\"nofollow\">https:&#x2F;&#x2F;dev.to&#x2F;n3rdh4ck3r&#x2F;how-to-give-claude-code-persistent...</a>",
        "created_at": "2026-02-19T19:37:40.000Z"
      }
    ]
  },
  "Show HN: Matrix OS – An AI operating system where Claude is the kernel": {
    "date": "2026-02-19T19:27:19Z",
    "url": "https://news.ycombinator.com/item?id=47077978",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "Matrix OS is an AI-native operating system: self-creating, self-healing, and self-expanding.<p>The Claude Agent SDK is the literal kernel:<p><pre><code>  CPU        =&gt; Claude Opus 4.6                                                                             \n  RAM        =&gt; Context window (1M tokens)                                                                  \n  Kernel     =&gt; Main agent + 26 IPC tools                                                                   \n  Processes  =&gt; 5 sub-agents\n  Disk       =&gt; ~&#x2F;apps, ~&#x2F;data, ~&#x2F;system\n  Syscalls   =&gt; Read, Write, Edit, Bash\n  Drivers    =&gt; MCP servers\n  IPC        =&gt; File coordination\n</code></pre>\nBreak something, the healer agent repairs it. Need a new capability, the OS writes its own agents and skills. Describe an app, it appears as a file you own.<p>The bigger vision is Web 4: OS + messaging + social + AI + identity, unified under one federated handle via Matrix protocol.<p>GitHub: github.com&#x2F;HamedMP&#x2F;matrix-os\nWhitepaper: matrix-os.com&#x2F;whitepaper<p>Happy to answer questions about the architecture.",
    "comments": []
  },
  "Show HN: Prodlint – A linter that catches what AI coding tools miss": {
    "date": "2026-02-19T18:44:02Z",
    "url": "https://news.ycombinator.com/item?id=47077393",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "I built Prodlint because I kept shipping the same bugs when building with Cursor, Copilot, and v0. Hardcoded secrets, missing auth checks, hallucinated imports, SQL injection through template literals — AI tools generate these constantly and TypeScript doesn&#x27;t catch them.<p>Prodlint is a zero-config static analysis tool with 52 rules across four categories: Security, Reliability, Performance, and AI Quality.\n  It runs in ~1 second, uses AST parsing (no LLM calls), and scores your codebase 0-100.<p>Some things it catches that surprised me during development:\n  - Imports for npm packages that don&#x27;t exist (AI hallucinates these)\n  - API methods that aren&#x27;t real (.flatten(), .contains(), .substr())\n  - &quot;use client&quot; on files that don&#x27;t need it\n  - Prisma writes without $transaction\n  - Next.js redirect() inside try&#x2F;catch (breaks silently)\n  - NEXT_PUBLIC_ on secrets like database URLs<p><pre><code>  Usage: npx prodlint (no install needed)\n</code></pre>\nAlso works as a GitHub Action (posts PR comments with scores) and as an MCP server for Claude Code &#x2F; Cursor &#x2F; Windsurf.<p>MIT licensed. Would love feedback on false positives — that&#x27;s the hardest part of building a linter",
    "comments": []
  },
  "Show HN: Rememex – Semantic file search that runs 100% locally (Rust/Tauri)": {
    "date": "2026-02-19T17:54:03Z",
    "url": "https://news.ycombinator.com/item?id=47076740",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "Hey HN, I built Rememex a semantic search layer for your local files.\nThe problem: I kept losing files. Not because they were deleted, but because \nI couldn&#x27;t remember the exact filename or keyword. grep needs the exact word. \nEverything only searches filenames. I wanted to type what I <i>meant</i> and find \nwhat I needed.\nHow it works:\n- Indexes 120+ file types (code, docs, images, configs)\n- Hybrid search: vector embeddings + full-text + JINA cross-encoder reranking\n- OCR on images via Windows UWP engine\n- Reads EXIF GPS → reverse geocodes to city names (&quot;photos from istanbul&quot; works)\n- EXIF dates → human language (&quot;summer morning&quot; finds a July 8am photo)\n- Smart chunking per language (Rust at fn&#x2F;struct, Python at def&#x2F;class)\n- Built-in MCP server so AI agents can use it as a tool\nEverything runs locally. Embeddings use a local ONNX model (Multilingual-E5-Base) \nby default, though you can optionally plug in OpenAI&#x2F;Gemini&#x2F;Cohere.\nNamed after Vannevar Bush&#x27;s Memex (1945) his vision of a device that stores \nand retrieves all human knowledge.\nStack: Rust (Tauri 2), React&#x2F;TypeScript, LanceDB, rayon\nI benchmarked it against grep for agentic tasks  rememex consistently finds \nthings in 1 step where grep takes 3-5 or fails entirely. The key difference: \ngrep needs the exact keyword, rememex needs the idea.\nWindows-only for now (UWP OCR dependency), but the core engine is portable.\nWould love feedback on the search quality and architecture. \nMIT licensed, free forever.",
    "comments": []
  },
  "Show HN: We Built an Open source MCP server to manage ads across 7 platforms": {
    "date": "2026-02-19T17:06:54Z",
    "url": "https://news.ycombinator.com/item?id=47076109",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "",
    "comments": []
  },
  "Show HN: Orchestera – Managed Apache Spark on Kubernetes in Your Own AWS Account": {
    "date": "2026-02-19T16:58:07Z",
    "url": "https://news.ycombinator.com/item?id=47075959",
    "matched_keywords": [
      "MCP server"
    ],
    "post": "I built Orchestera as a PaaS that allows you to orchestrate Apache Spark clusters in your own AWS account, with no additional markup on compute via EC2 instances.<p>I built this because I was tired of the compute markup that products like AWS EMR and Databricks charge for the convenience of using Apache Spark via their platforms. One can argue that Databricks is a superior product with a lot of additional value in their offering but I don&#x27;t see that with AWS EMR Apache Spark at all (given my personal experience working with it).<p>My motivation to build this was to be able to create your own Apache Spark cluster without needing any understanding of the underlying data infrastructure engineering and quickly get to the point of writing Spark pipelines, whether as Python applications or Jupyter notebooks, all with no markup on compute because I don&#x27;t think that is a justified narrative.<p>It took me almost an year to build it with a day job and of course I used AI for frontend design and video narrations, the infrastructue engineering that goes behind it comes with quite a bit of experience in the industry. The backend that orchestrates the cluster is written with the following:<p>- Django and DRF for API<p>- Temporal for async workers<p>- Pulumi that is run via Temporal workers to orchestrate the cluster<p>- Karpenter for node auto-scaling based on Spark executor workloads and requests<p>- Librechat for Spark History server and MCP based debugging for Spark pipeline run analysis<p>There are currently no caps on the CPU limits so you can try this out today in your own personal AWS accounts for free.<p>Also looking for feedback on HN.",
    "comments": [
      {
        "author": "jazib",
        "text": "This looks super cool. Going to give it a spin!",
        "created_at": "2026-02-19T17:19:05.000Z"
      }
    ]
  },
  "Show HN: Crit – Visual QA for iOS apps and AI coding agents": {
    "date": "2026-02-19T15:59:18Z",
    "url": "https://news.ycombinator.com/item?id=47075182",
    "matched_keywords": [
      "MCP server"
    ],
    "post": "I built Crit, a CLI tool that lets you capture screenshots from iOS Simulator, drop pins on what&#x27;s wrong, and hand structured feedback to any coding agent.<p>You just:<p>crit capture — screenshot your app screens\ncrit serve — review in browser, click to pin bugs and add comments\nTell your agent: &quot;review .crit and fix each issue&quot;<p>It saves annotated screenshots and JSON to a .crit&#x2F; folder. Works with Claude Code, Cursor, Codex, Gemini — anything that can read images. No plugins, no MCP, no dependencies.<p>macOS + Xcode required. Android not yet supported.\nRepo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;natethegreat&#x2F;crit\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;natethegreat&#x2F;crit</a>",
    "comments": []
  },
  "Powering the next generation of agents with Google Cloud databases": {
    "date": "2026-02-19T15:53:23Z",
    "url": "https://news.ycombinator.com/item?id=47075116",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "",
    "comments": []
  },
  "Ochat – reproducible, diffable LLM workflows in a single Markdown file": {
    "date": "2026-02-19T15:08:07Z",
    "url": "https://news.ycombinator.com/item?id=47074581",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "I built Ochat, a toolkit for building AI agent workflows out of a small set of primitives.<p>The core primitive is ChatMarkdown (ChatMD): a single .md file is both:<p>the prompt&#x2F;program (model config, tool allowlist, instructions, context), and\nthe auditable transcript (assistant replies + tool calls + tool outputs)\nThe part that feels most powerful in practice is that this simple building block scales: with good prompting + a curated tool set you can build lots of workflows, and then package them as prompt packs by mounting other prompts as tools (“agent-as-tool”). That lets you assemble Claude Code&#x2F;Codex-style “agent apps” as just a folder of .md files.<p>High-leverage built-ins (especially for coding workflows):<p>apply_patch (repo-safe atomic edits)\nread_file &#x2F; read_dir (safe grounding in local files)\nwebpage_to_markdown (web ingestion + GitHub blob fast-path)\nlocal retrieval: index_markdown_docs + markdown_search\ncode retrieval: index_ocaml_code + query_vector_db\nimport_image (vision inputs)\nExtensibility: beyond built-ins, you can add narrowly-scoped shell wrappers, and (optionally) import external tools via MCP. MCP isn’t the point of the project, but it’s useful when you want to reuse existing tool servers.<p>You can run the same prompt file via:<p>chat_tui (interactive terminal UI; persistent sessions; branching&#x2F;export; manual context compaction)\nochat chat-completion (scripts&#x2F;CI)\nmcp_server (expose prompts as MCP tools)\nCaveats: provider support today is OpenAI-only; project is research-grade and evolving quickly.<p>Repo\n&lt;https:&#x2F;&#x2F;github.com&#x2F;dakotamurphyucf&#x2F;ochat&gt;<p>Demo\n&lt;https:&#x2F;&#x2F;youtu.be&#x2F;eGgmUdZfnxM&gt;<p>If this resonates: stars help a lot, and I’d love early adopters + contributors (prompt packs, examples, docs, tool integrations).<p>Minimal snippet (prompt pack orchestrator + optional MCP tool):<p><pre><code>  &lt;config model=&quot;gpt-5.2&quot; reasoning_effort=&quot;medium&quot; temperature=&quot;0&quot;&#x2F;&gt;\n\n  &lt;!-- core built-ins --&gt;\n  &lt;tool name=&quot;read_dir&quot;&#x2F;&gt;\n  &lt;tool name=&quot;read_file&quot;&#x2F;&gt;\n  &lt;tool name=&quot;apply_patch&quot;&#x2F;&gt;\n  &lt;tool name=&quot;webpage_to_markdown&quot;&#x2F;&gt;\n\n  &lt;!-- optional: import an external tool via MCP --&gt;\n  &lt;tool mcp_server=&quot;stdio:npx -y brave-search-mcp&quot; name=&quot;brave_web_search&quot; &#x2F;&gt;\n\n  &lt;!-- prompt-pack tools (agents as tools) --&gt;\n  &lt;tool name=&quot;plan&quot;   agent=&quot;prompts&#x2F;pack&#x2F;plan.md&quot; local&#x2F;&gt;\n  &lt;tool name=&quot;code&quot;   agent=&quot;prompts&#x2F;pack&#x2F;code.md&quot; local&#x2F;&gt;\n  &lt;tool name=&quot;review&quot; agent=&quot;prompts&#x2F;pack&#x2F;review.md&quot; local&#x2F;&gt;\n\n  &lt;developer&gt;\n  You are the orchestrator. Call plan first.\n  Keep edits small. Before apply_patch: explain the diff and wait for confirmation.\n  &lt;&#x2F;developer&gt;\n\n  &lt;user&gt;\n  Add a Quickstart section to README.md.\n  &lt;&#x2F;user&gt;</code></pre>",
    "comments": []
  },
  "Show HN: EasyMemory – 100% local memory layer and MCP for LLMs": {
    "date": "2026-02-19T14:08:49Z",
    "url": "https://news.ycombinator.com/item?id=47073880",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "Hi everyone,\nI have created EasyMemory: a lightweight, fully local memory backend for chatbots, agents and any MCP-compatible LLM (Claude, GPT, Gemini, Ollama…).\nKey points:\n•  Auto-saves every conversation\n•  Ingests PDFs, DOCX, Markdown vaults, folders\n•  Hybrid retrieval: vector + keyword + graph (no extra libs needed)\n•  Built-in MCP server → plug into Claude Desktop, custom agents, etc.\n•  100% offline, data in ~&#x2F;.easymemory\n•  Enterprise extras: OAuth2, API keys, rate limiting, audit logs\n•  Bonus: Slack JSON import, Notion&#x2F;GDrive folder indexing\nQuick start (MCP server):<p>easymemory-server --port 8100<p>Then point Claude Desktop or your agent to http:&#x2F;&#x2F;localhost:8100&#x2F;mcp.\nOr chat with Ollama:<p>easymemory-agent --provider ollama --model llama3.1:8b<p>Python usage:<p>from easymemory.agent import EasyMemoryAgent\nasync with EasyMemoryAgent(llm_provider=&quot;ollama&quot;, model=&quot;llama3.1:8b&quot;) as agent:\n    print(await agent.chat(&quot;Remember: I prefer dark mode.&quot;))\n    # Later...\n    print(await agent.chat(&quot;What UI do I prefer?&quot;))  # → &quot;You prefer dark mode&quot;<p>MIT licensed, minimal deps, early stage.\nRepo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;JustVugg&#x2F;easymemory\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;JustVugg&#x2F;easymemory</a>\nLooking for feedback on:\n•  What retrieval mix works best for your long-term memory needs?\n•  Pain points with current local memory solutions?\n•  Nice-to-have integrations?\nThanks!",
    "comments": []
  },
  "Show HN: LLM-use – cost-effective LLM orchestrator for agents": {
    "date": "2026-02-19T13:59:01Z",
    "url": "https://news.ycombinator.com/item?id=47073778",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "Hi HN,\nBuilt llm-use: a lightweight Python toolkit for efficient agent workflows with multiple LLMs.\nCore pattern: strong model (Claude&#x2F;GPT-4o&#x2F;big local) for planning + synthesis; cheap&#x2F;local workers for parallel subtasks (research, scrape, summarize, extract…).\nFeatures:\n•  Mix Anthropic, OpenAI, Ollama, llama.cpp\n•  Smart router: cheap&#x2F;local first, escalate only if needed (learned + heuristic)\n•  Parallel workers (–max-workers)\n•  Real scraping + cache (BS4 or Playwright)\n•  Offline-first (full Ollama support)\n•  Cost tracking ($ for cloud, 0 local)\n•  TUI chat + MCP server mode\n•  Local session logs\nQuick example (hybrid):<p>python3 cli.py exec \\\n  --orchestrator anthropic:claude-3-7-sonnet-20250219 \\\n  --worker ollama:llama3.1:8b \\\n  --enable-scrape \\\n  --task &quot;Summarize 6 recent sources on post-quantum crypto&quot;<p>Or routed version:<p>python3 cli.py exec \\\n  --router ollama:llama3.1:8b \\\n  --orchestrator openai:o1 \\\n  --worker gpt-4o-mini \\\n  --task &quot;Explain recent macOS security updates&quot;<p>MIT licensed, minimal deps, embeddable.\nRepo: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;llm-use&#x2F;llm-use\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;llm-use&#x2F;llm-use</a>\nFeedback welcome on:\n•  Routing heuristics you’d find useful\n•  Pain points with agent costs &#x2F; local vs cloud\n•  Missing integrations?\nThanks!",
    "comments": []
  },
  "Show HN: Synter- Open source MCP server to manage ads across 7 platforms": {
    "date": "2026-02-19T13:07:48Z",
    "url": "https://news.ycombinator.com/item?id=47073329",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "",
    "comments": [
      {
        "author": "synterai",
        "text": "Link here - <a href=\"https:&#x2F;&#x2F;github.com&#x2F;jshorwitz&#x2F;synter-mcp-server\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;jshorwitz&#x2F;synter-mcp-server</a>",
        "created_at": "2026-02-19T17:05:53.000Z"
      }
    ]
  },
  "Show HN: Unix-style pipeline composition for MCP tool calls": {
    "date": "2026-02-19T12:46:54Z",
    "url": "https://news.ycombinator.com/item?id=47073185",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "This is a little side-project I have been working on at my job.<p>Model Context Shell lets AI agents compose MCP tool calls using something similar to Unix shell scripting. Instead of the agent making each tool call individually (loading all intermediate data into context), it can express a workflow as a pipeline that executes server-side.<p>Since the orchestration is deterministic and reproducible, you can also use it with Skills.<p>Tool orchestration runs outside the agent and LLM context, so the agent can extract only the relevant parts of data and load those into context. This means you can save tokens, but also you can work with data that is too big to load into context, and your agent can trigger a very large number of tool calls if needed.<p>Also, this is not just a tool that runs bash - it has its own execution engine. So no need for full system access.<p>Example query: &quot;List all Pokemon over 50 kg that have the chlorophyll ability&quot;<p>Instead of 7+ separate tool calls loading all Pokemon data into context, the agent builds a single pipeline that:<p>1. Fetches the ability data\n2. Extracts Pokemon URLs\n3. Fetched each Pokemon&#x27;s details (7 tool calls)\n4. Filters by weight and formats the results<p>At least in it&#x27;s current iteration, it&#x27;s packaged as an MCP server itself. So you can use it with any agent. I made this, and some other design choices, so you can try it right away.",
    "comments": []
  },
  "Show HN: Agent skills to build photo, video and design editors on the web": {
    "date": "2026-02-19T12:23:02Z",
    "url": "https://news.ycombinator.com/item?id=47073035",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "This claude code plugin and npx skill bundles the full CE.SDK documentation, guided code generation, and a builder agent that scaffolds complete photo&#x2F;video&#x2F;design editor projects from scratch, all offline, no API calls or MCP servers needed.<p>Supports 10 frameworks: React, Vue, Svelte, Angular, Next.js, Nuxt.js, SvelteKit, Electron, Node.js, and vanilla JS.",
    "comments": []
  },
  "Show HN: Hydra – A safer OpenClaw alternative using containerized agents": {
    "date": "2026-02-19T12:13:14Z",
    "url": "https://news.ycombinator.com/item?id=47072965",
    "matched_keywords": [
      "mcp-server"
    ],
    "post": "Hey HN!<p>I&#x27;m a pentester, and the recent wave of security issues with AI agent frameworks (exposed API keys, RCE vulnerabilities, malicious marketplace plugins) made me uncomfortable enough to build something different.<p>Hydra runs every AI agent inside its own container. Agents start with nothing, and only sees what you explicitly declare (mounts, secrets, etc). Mounts and secrets require agreement between two independent config files (the agent config and a separate host-level allowlist), so even if an agent&#x27;s config gets tampered with, it can&#x27;t escalate its own access.<p>Two modes of interaction:<p>- `hydra exec` gives you a full interactive Claude Code session inside the restricted agent container<p>- Orchestrated mode for automation: agents communicate via filesystem-based IPC for things like Telegram bots or scheduled tasks<p>The project was inspired by NanoClaw and completely redesigned to support contained Claude Code sessions with per-agent mounts, secrets, and MCP servers.<p>You can find the repo here: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;RickConsole&#x2F;hydra\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;RickConsole&#x2F;hydra</a> and the Readme has the link to the writeup for it.<p>Happy to answer any questions about the architecture or threat model!",
    "comments": []
  },
  "Show HN: Agorio – TypeScript SDK for Building AI Shopping Agents (UCP/ACP)": {
    "date": "2026-02-19T11:48:07Z",
    "url": "https://news.ycombinator.com/item?id=47072813",
    "matched_keywords": [
      "MCP server"
    ],
    "post": "I built an open-source TypeScript SDK for building AI agents that can discover merchants, browse products, and complete purchases using the new UCP (Google&#x2F;Shopify) and ACP (OpenAI&#x2F;Stripe) commerce protocols.<p>Try it in 2 minutes:<p><pre><code>  npm install @agorio&#x2F;sdk\n\n  import { ShoppingAgent, GeminiAdapter, MockMerchant } from &#x27;@agorio&#x2F;sdk&#x27;;\n  const merchant = new MockMerchant();\n  await merchant.start();\n  const agent = new ShoppingAgent({\n    llm: new GeminiAdapter({ apiKey: process.env.GEMINI_API_KEY })\n  });\n  const result = await agent.run(\n    `Go to ${merchant.domain} and buy me wireless headphones`\n  );\n</code></pre>\nWhat it does:<p>- UcpClient: discovers merchants via &#x2F;.well-known&#x2F;ucp, parses capabilities, normalizes both array and object formats, calls REST APIs\n- ShoppingAgent: plan-act-observe loop with 12 built-in tools (discover, search, browse, cart, checkout, order tracking)\n- MockMerchant: full UCP-compliant Express server with product catalog, checkout flow, and configurable chaos testing (latency, error rates)\n- LlmAdapter interface: swap LLMs without changing agent code. Gemini ships today, Claude and OpenAI coming in v0.2<p>The agent handles the entire purchase flow autonomously - UCP discovery, product search, cart management, shipping, payment, order confirmation. 37 tests passing.<p>Context: UCP was announced Jan 11 by Google, Shopify, and 25+ partners (Walmart, Target, Visa, Mastercard). ACP is by OpenAI and Stripe, powers ChatGPT Instant Checkout. Both are open standards. But there was no developer SDK for building on top of them - just the raw specs.<p>GitHub: <a href=\"https:&#x2F;&#x2F;github.com&#x2F;Nolpak14&#x2F;agorio\" rel=\"nofollow\">https:&#x2F;&#x2F;github.com&#x2F;Nolpak14&#x2F;agorio</a>\nnpm: <a href=\"https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;@agorio&#x2F;sdk\" rel=\"nofollow\">https:&#x2F;&#x2F;www.npmjs.com&#x2F;package&#x2F;@agorio&#x2F;sdk</a>",
    "comments": [
      {
        "author": "nolpak14",
        "text": "Hey HN, I&#x27;m the author.<p>Some background: I&#x27;ve been working on tooling for UCP (Universal Commerce Protocol) for a few months. UCP is the open standard Google and Shopify announced in January - it lets AI agents discover stores via &#x2F;.well-known&#x2F;ucp and complete purchases through standardized APIs.<p>I built Agorio because when I tried to build a shopping agent against the UCP spec, I had to:<p>1. Write my own profile parser that handles both capability formats in the spec\n2. Build a checkout state machine (incomplete → requires_escalation → ready_for_complete → completed)\n3. Create a mock merchant from scratch just to test against\n4. Wire up LLM function calling with JSON Schema tool definitions<p>None of this was commerce-specific - it was all protocol plumbing. So I extracted it into a reusable SDK.<p>The key abstractions:<p>- LlmAdapter - two methods: chat(messages, tools) and modelName. Any LLM with function calling works. The Gemini adapter is ~100 lines.\n- ShoppingAgent - takes an LlmAdapter, runs plan-act-observe with 12 tools. Manages cart state, checkout sessions, order history.\n- UcpClient - fetches &#x2F;.well-known&#x2F;ucp, normalizes capabilities, resolves REST&#x2F;MCP&#x2F;A2A transports.\n- MockMerchant - full Express server with UCP profile, OpenAPI schema, 10 products, checkout flow, order tracking. Supports chaos testing with configurable latency and error rates.<p>Technical choices I&#x27;d like feedback on:<p>- Is a plan-act-observe loop the right pattern, or should I support ReAct &#x2F; tree-of-thought?\n- Currently UCP-only. ACP client is planned for v0.2. Should I prioritize that?\n- The LlmAdapter interface is deliberately minimal. Too minimal?<p>Would love feedback from anyone building with LLM function calling or commerce APIs. Happy to talk UCP&#x2F;ACP protocol details.",
        "created_at": "2026-02-19T11:49:06.000Z"
      }
    ]
  }
}